{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Efficient Estimation of Word Representations in Vector Space \\(Mikolov et al., 2013\\)](https://arxiv.org/pdf/1301.3781.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why use uniform initialization instead of xavier uniform for word embeddings? Why do we pick the range [-0.25, 0.25]?\n",
    "\n",
    "[Weight initialization in neural nets](https://kharshit.github.io/blog/2019/02/08/weight-initialization-in-neural-nets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class WordEmbCbow(torch.nn.Module):\n",
    "    def __init__(self, nwords, emb_size):\n",
    "        super(WordEmbCbow, self).__init__()\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(nwords, emb_size)\n",
    "        \n",
    "        # Fills the input Tensor with values drawn \n",
    "        # from the uniform distribution U(a, b)\n",
    "        torch.nn.init.uniform_(self.embedding.weight, -0.25, 0.25)\n",
    "        \n",
    "        # Projection layer for taking softmax over vocab words\n",
    "        self.projection = torch.nn.Linear(emb_size, nwords)\n",
    "        torch.nn.init.uniform_(self.projection.weight, -0.25, 0.25)\n",
    "        \n",
    "    def forward(self, words):\n",
    "        emb = self.embedding(words)\n",
    "        emb_sum = torch.sum(emb, dim=0) # size = emb_size\n",
    "        emb_sum = emb_sum.view(1, -1) # size = 1 x emb_size \n",
    "        out = self.projection(emb_sum)\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2  # length of window on each side (so N=2 gives a total window size of 5, as in t-2 t-1 t t+1 t+2)\n",
    "EMB_SIZE = 128  # The size of the embedding\n",
    "\n",
    "embeddings_location = \"embeddings.txt\"  # the file to write the word embeddings to\n",
    "labels_location = \"labels.txt\"  # the file to write the labels to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "S = w2i[\"<s>\"]\n",
    "UNK = w2i[\"<unk>\"]\n",
    "\n",
    "\n",
    "def read_dataset(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            yield [w2i[x] for x in line.strip().split(\" \")]\n",
    "\n",
    "\n",
    "# Read in the data\n",
    "train = list(read_dataset(\"../data/ptb/train.txt\"))\n",
    "w2i = defaultdict(lambda: UNK, w2i)\n",
    "dev = list(read_dataset(\"../data/ptb/valid.txt\"))\n",
    "i2w = {v: k for k, v in w2i.items()}\n",
    "nwords = len(w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "if not os.path.exists(labels_location):\n",
    "    with open(labels_location, 'w') as labels_file:\n",
    "        for i in range(nwords):\n",
    "            labels_file.write(i2w[i] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = WordEmbCbow(nwords, EMB_SIZE)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "data_type = torch.LongTensor\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    data_type = torch.cuda.LongTensor\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate loss for entire sentence\n",
    "def calc_sent_loss(sent):\n",
    "    # Add padding to sentence equal to the size of the window\n",
    "    padding = [S]*N\n",
    "    padded_sent = padding + sent + padding \n",
    "    \n",
    "    losses = []\n",
    "    for i in range(N, len(sent)+N):\n",
    "        # c is the context vector (does not include actual token)\n",
    "        before = padded_sent[i-N:i]\n",
    "        after = padded_sent[i+1:i+N+1]\n",
    "        c = torch.tensor(before+after).type(data_type)\n",
    "        \n",
    "        # t is the target vector\n",
    "        t = torch.tensor([padded_sent[i]]).type(data_type)\n",
    "        \n",
    "        logits = model(c)\n",
    "        loss = criterion(logits, t)  # loss for predicting target from context\n",
    "        losses.append(loss)\n",
    "    return torch.stack(losses).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started iter 0\n",
      "--finished 5000 sentences\n",
      "--finished 10000 sentences\n",
      "--finished 15000 sentences\n",
      "--finished 20000 sentences\n",
      "--finished 25000 sentences\n",
      "--finished 30000 sentences\n",
      "--finished 35000 sentences\n",
      "--finished 40000 sentences\n",
      "iter 0: train loss/word=nan, ppl=nan, time=403.1145212650299\n",
      "iter 0: train loss/word=nan, ppl=nan, time=18.368483543395996\n",
      "saving embedding files\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'embedings_location' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-70a65ef59146>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saving embedding files\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedings_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mW_w_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embedings_location' is not defined"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 100\n",
    "\n",
    "for ITER in range(1):\n",
    "    print(\"started iter %r\" % ITER)\n",
    "    \n",
    "    # Start training\n",
    "    random.shuffle(train)\n",
    "    train_words, train_loss = 0, 0.0\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    \n",
    "    for sent_id, sent in enumerate(train):\n",
    "        my_loss = calc_sent_loss(sent)\n",
    "        train_loss += my_loss.item()\n",
    "        train_words += len(sent)\n",
    "        \n",
    "        # Take step after calculating loss for all words in sent\n",
    "        optimizer.zero_grad()  # Zero the gradients \n",
    "        my_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (sent_id+1) % 5000 == 0:\n",
    "            print(\"--finished {} sentences\".format(sent_id+1))\n",
    "    \n",
    "    print(\"iter {}: train loss/word={}, ppl={}, time={}\".format(\n",
    "        ITER,\n",
    "        train_loss/train_words,\n",
    "        math.exp(train_loss/train_words),\n",
    "        time.time()-start))\n",
    "    \n",
    "    # Evaluate on dev set \n",
    "    dev_words, dev_loss = 0, 0.0\n",
    "    start = time.time()\n",
    "    model.eval() \n",
    "    for sent_id, sent in enumerate(dev):\n",
    "        my_loss = calc_sent_loss(sent)\n",
    "        dev_loss += my_loss.item()\n",
    "        dev_words += len(sent)\n",
    "        \n",
    "    print(\"iter {}: dev loss/word={}, ppl={}, time={}\".format(\n",
    "        ITER,\n",
    "        dev_loss/dev_words,\n",
    "        math.exp(dev_loss/dev_words),\n",
    "        time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving embedding files\n"
     ]
    }
   ],
   "source": [
    "print(\"saving embedding files\")\n",
    "with open(embeddings_location, \"w\") as outfile:\n",
    "    W_w_np = model.embedding.weight.data.cpu().numpy()\n",
    "    for i in range(nwords):\n",
    "        ith_embedding = \"\\t\".join(map(str, W_w_np[i]))\n",
    "        outfile.write(ith_embedding + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn4nlp",
   "language": "python",
   "name": "nn4nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
