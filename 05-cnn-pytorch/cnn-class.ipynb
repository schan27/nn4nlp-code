{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import time\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to read in the corpus\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "t2i = defaultdict(lambda: len(t2i))\n",
    "UNK = w2i[\"<unk>\"]\n",
    "\n",
    "\n",
    "def read_dataset(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            tag, words = line.lower().strip().split(\" ||| \")\n",
    "            yield ([w2i[x] for x in words.split(\" \")], t2i[tag])\n",
    "\n",
    "\n",
    "# Read in the data\n",
    "train = list(read_dataset(\"../data/classes/train.txt\"))\n",
    "w2i = defaultdict(lambda: UNK, w2i)\n",
    "dev = list(read_dataset(\"../data/classes/test.txt\"))\n",
    "nwords = len(w2i)\n",
    "ntags = len(t2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Dilated convolution](https://towardsdatascience.com/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [`torch.nn.Conv1d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Stanford CS224N: NLP with Deep Learning | Winter 2019 | Lecture 11 â€“ Convolutional Networks for NLP](https://www.youtube.com/watch?v=EAJoRA0KX7I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Padding by deep.ai](https://www.youtube.com/watch?v=smHa2442Ah4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClass(torch.nn.Module):\n",
    "    def __init__(self, nwords, emb_size, num_filters, window_size, ntags):\n",
    "        super(CNNClass, self).__init__()\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(nwords, emb_size)\n",
    "        \n",
    "        torch.nn.init.uniform_(self.embedding.weight, -0.25, 0.25)\n",
    "        \n",
    "        # Conv 1d\n",
    "        self.conv_1d = torch.nn.Conv1d(\n",
    "            in_channels=emb_size,\n",
    "            out_channels=num_filters,\n",
    "            kernel_size=window_size,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            dilation=1,\n",
    "            groups=1,\n",
    "            bias=True)\n",
    "        \n",
    "        self.relu = torch.nn.ReLU()  # Activation function\n",
    "        \n",
    "        # Project num_filters onto ntags\n",
    "        self.projection_layer = torch.nn.Linear(\n",
    "            in_features=num_filters,\n",
    "            out_features=ntags,\n",
    "            bias=True)\n",
    "        \n",
    "        # Initializing the projection layer\n",
    "        torch.nn.init.xavier_uniform_(self.projection_layer.weight)\n",
    "        \n",
    "    def forward(self, words):\n",
    "        emb = self.embedding(words)  # nwords x emb_size \n",
    "        emb = emb.unsqueeze(0).permute(0, 2, 1)  # 1 x emb_size x n_words \n",
    "        h = self.conv_1d(emb) # 1 x num_filters x n_words\n",
    "        \n",
    "        # Do max pooling\n",
    "        h = h.max(dim=2)[0]  # 1 x num_filters\n",
    "        h = self.relu(h)     \n",
    "        out = self.projection_layer(h) # 1 x ntags\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "EMB_SIZE = 64\n",
    "WIN_SIZE = 3\n",
    "FILTER_SIZE = 64\n",
    "\n",
    "# initialize the model\n",
    "model = CNNClass(nwords, EMB_SIZE, FILTER_SIZE, WIN_SIZE, ntags)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "data_type = torch.LongTensor\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    data_type = torch.cuda.LongTensor\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: train loss/sent=0.8887445103056199, acc=0.18422284644194756, time=13.640254259109497\n",
      "iter 0: test acc=0.34479638009049773\n"
     ]
    }
   ],
   "source": [
    "for ITER in range(1):\n",
    "    # Perform training\n",
    "    random.shuffle(train)\n",
    "    train_loss, train_correct = 0.0, 0\n",
    "    start = time.time()\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for words, tag in train[:5000]:\n",
    "        if len(words) < WIN_SIZE:\n",
    "            # Add padding\n",
    "            words += [0] * (WIN_SIZE - len(words))\n",
    "            \n",
    "        words_tensor = torch.tensor(words).type(data_type)\n",
    "        tag_tensor = torch.tensor([tag]).type(data_type)\n",
    "        scores = model(words_tensor)\n",
    "        predict = scores[0].argmax().item()\n",
    "        \n",
    "        if predict == tag:\n",
    "            train_correct += 1\n",
    "        \n",
    "        my_loss = criterion(scores, tag_tensor)\n",
    "        train_loss += my_loss.item()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.zero_grad()\n",
    "        my_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(\"iter {}: train loss/sent={}, acc={}, time={}\".format(\n",
    "        ITER,\n",
    "        train_loss/len(train),\n",
    "        train_correct/len(train),\n",
    "        time.time()-start))\n",
    "    \n",
    "    test_correct = 0\n",
    "    model.eval()\n",
    "    \n",
    "    for words, tag in dev[:5000]:\n",
    "        # Padding \n",
    "        if len(words) < WIN_SIZE:\n",
    "            words += [0] * (WIN_SIZE - len(words))\n",
    "        words_tensor = torch.tensor(words).type(data_type)\n",
    "        scores = model(words_tensor)[0]\n",
    "        predict = scores.argmax().item()\n",
    "        \n",
    "        if predict == tag:\n",
    "            test_correct += 1\n",
    "    \n",
    "    print(\"iter {}: test acc={}\".format(\n",
    "        ITER,\n",
    "        test_correct/len(dev)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn4nlp",
   "language": "python",
   "name": "nn4nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
