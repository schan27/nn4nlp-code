{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClass(torch.nn.Module):\n",
    "    def __init__(self, nwords, emb_size, num_filters, window_size, ntags):\n",
    "        super(CNNClass, self).__init__()\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(nwords, emb_size)\n",
    "        torch.nn.init.uniform_(self.embedding.weight, -0.25, 0.25)\n",
    "        \n",
    "        self.conv_1d = torch.nn.Conv1d(\n",
    "            in_channels=emb_size,\n",
    "            out_channels=num_filters,\n",
    "            kernel_size=window_size,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            dilation=1,\n",
    "            groups=1,\n",
    "            bias=True)\n",
    "        \n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "        self.projection_layer = torch.nn.Linear(\n",
    "            in_features=num_filters,\n",
    "            out_features=ntags,\n",
    "            bias=True)\n",
    "        \n",
    "        torch.nn.init.xavier_uniform_(self.projection_layer.weight)\n",
    "    \n",
    "    def forward(self, words, return_activations=False):\n",
    "        emb = self.embedding(words) # nwords x emb_size\n",
    "        \n",
    "        # Use `permute` to switch channel positions\n",
    "        emb = emb.unsqueeze(0).permute(0, 2, 1) # 1 x emb_size x n_words\n",
    "        \n",
    "        h = self.conv_1d(emb) # 1 x num_filters x nwords\n",
    "        \n",
    "        # Argmax along length of the sentence\n",
    "        activations = h.squeeze(0).max(dim=1)[1]\n",
    "        \n",
    "        # Do max pooling\n",
    "        h = h.max(dim=2)[0]\n",
    "        h = self.relu(h)\n",
    "        features = h.squeeze(0)\n",
    "        out = self.projection_layer(h) # 1 x ntags\n",
    "        \n",
    "        if return_activations:\n",
    "            return out, activations.data.cpu().numpy(), features.data.cpu().numpy()\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "np.set_printoptions(linewidth=sys.maxsize, threshold=sys.maxsize)\n",
    "\n",
    "# Functions to read in the corpus\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "UNK = w2i[\"<unk>\"]\n",
    "def read_dataset(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            tag, words = line.lower().strip().split(\" ||| \")\n",
    "            words = words.split(\" \")\n",
    "            yield (words, [w2i[x] for x in words], int(tag))\n",
    "\n",
    "\n",
    "# Read in the data\n",
    "train = list(read_dataset(\"../data/classes/train.txt\"))\n",
    "w2i = defaultdict(lambda: UNK, w2i)\n",
    "dev = list(read_dataset(\"../data/classes/test.txt\"))\n",
    "nwords = len(w2i)\n",
    "ntags = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "EMB_SIZE = 10\n",
    "WIN_SIZE = 3\n",
    "FILTER_SIZE = 8\n",
    "\n",
    "# initialize the model\n",
    "model = CNNClass(nwords, EMB_SIZE, FILTER_SIZE, WIN_SIZE, ntags)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = torch.LongTensor\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    data_type = torch.cuda.LongTensor\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_predict_and_activations(wids, tag, words):\n",
    "    \n",
    "    \n",
    "    def display_activations(words, activations):\n",
    "        pad_begin = (WIN_SIZE - 1) / 2\n",
    "        pad_end = WIN_SIZE - 1 - pad_begin \n",
    "        words_padded = [\"pad\"] * int(pad_begin) + words + [\"pad\"] * int(pad_end)\n",
    "        \n",
    "        ngrams = []\n",
    "        for act in activations:\n",
    "            act = int(act)\n",
    "            ngrams.append(\n",
    "                \"[\" + \n",
    "                \", \".join(words_padded[act:act+WIN_SIZE]) \n",
    "                + \"]\") \n",
    "        return ngrams\n",
    "            \n",
    "    \n",
    "    # Padding\n",
    "    if len(wids) < WIN_SIZE:\n",
    "        wids += [0] * (WIN_SIZE-len(wids))\n",
    "        \n",
    "    words_tensor = torch.tensor(wids).type(data_type)\n",
    "    scores, activations, features = model(\n",
    "        words_tensor, return_activations=True)\n",
    "    scores = scores.squeeze().cpu().data.numpy()\n",
    "    \n",
    "    print('%d ||| %s' % (tag, ' '.join(words)))\n",
    "    predict = np.argmax(scores)\n",
    "    \n",
    "    print(display_activations(words, activations))\n",
    "    W = model.projection_layer.weight.data.cpu().numpy() # Weight matrix\n",
    "    bias = model.projection_layer.bias.data.cpu().numpy()\n",
    "    \n",
    "    print(\"scores={}, predict: {}\".format(scores, predict))\n",
    "    print('  bias={}'.format(bias))\n",
    "    \n",
    "    contributions = W * features\n",
    "    print(' very bad (%.4f): %s' % (scores[0], contributions[0]))\n",
    "    print('      bad (%.4f): %s' % (scores[1], contributions[1]))\n",
    "    print('  neutral (%.4f): %s' % (scores[2], contributions[2]))\n",
    "    print('     good (%.4f): %s' % (scores[3], contributions[3]))\n",
    "    print('very good (%.4f): %s' % (scores[4], contributions[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: train loss/sent=0.9165, acc=0.1635, time=16.64s\n",
      "iter 0: test acc=0.2308\n"
     ]
    }
   ],
   "source": [
    "for ITER in range(1):\n",
    "    random.shuffle(train)\n",
    "    \n",
    "    train_loss, train_correct = 0.0, 0.0\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    \n",
    "    for _, wids, tag in train[:5000]:\n",
    "        if len(wids) < WIN_SIZE:\n",
    "            wids += [0] * (WIN_SIZE - len(wids))\n",
    "        words_tensor = torch.tensor(wids).type(data_type)\n",
    "        tag_tensor = torch.tensor([tag]).type(data_type)\n",
    "        scores = model(words_tensor)\n",
    "        predict = scores[0].argmax().item()\n",
    "        if predict == tag:\n",
    "            train_correct += 1\n",
    "\n",
    "        my_loss = criterion(scores, tag_tensor)\n",
    "        train_loss += my_loss.item()\n",
    "        # Do back-prop\n",
    "        optimizer.zero_grad()\n",
    "        my_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(\"iter %r: train loss/sent=%.4f, acc=%.4f, time=%.2fs\" % (ITER, train_loss/len(train), train_correct/len(train), time.time()-start))\n",
    "    \n",
    "    # Testing\n",
    "    test_correct = 0.0\n",
    "    for _, wids, tag in dev[:5000]:\n",
    "        # Padding (can be done in the conv layer as well)\n",
    "        if len(wids) < WIN_SIZE:\n",
    "            wids += [0] * (WIN_SIZE - len(wids))\n",
    "        words_tensor = torch.tensor(wids).type(data_type)\n",
    "        scores = model(words_tensor)\n",
    "        predict = scores[0].argmax().item()\n",
    "        if predict == tag:\n",
    "            test_correct += 1\n",
    "            \n",
    "    print(\"iter %r: test acc=%.4f\" % (ITER, test_correct/len(dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 ||| effective but too-tepid biopic\n",
      "['[effective, but, too-tepid]', '[pad, effective, but]', '[pad, effective, but]', '[effective, but, too-tepid]', '[pad, effective, but]', '[pad, effective, but]', '[effective, but, too-tepid]', '[pad, effective, but]']\n",
      "scores=[-0.08909433  0.28088307  0.3214271   0.6062926  -0.15508379], predict: 3\n",
      "  bias=[ 0.05086858  0.16146143 -0.0737052   0.16508259  0.01069979]\n",
      " very bad (-0.0891): [ 0.0143822   0.          0.005133    0.          0.01145995  0.          0.         -0.17093806]\n",
      "      bad (0.2809): [-0.1514776  -0.          0.1721953   0.          0.04248144  0.         -0.          0.05622251]\n",
      "  neutral (0.3214): [ 0.19533098 -0.          0.02746848 -0.          0.08763424  0.          0.          0.0846986 ]\n",
      "     good (0.6063): [ 0.08825286 -0.          0.10560799 -0.          0.13073145 -0.         -0.          0.11661772]\n",
      "very good (-0.1551): [-0.15279694  0.          0.03289231  0.          0.02185056  0.         -0.         -0.0677295 ]\n",
      "\n",
      "3 ||| if you sometimes like to go to the movies to have fun , wasabi is a good place to start .\n",
      "['[a, good, place]', '[if, you, sometimes]', '[a, good, place]', '[a, good, place]', '[wasabi, is, a]', '[have, fun, ,]', '[pad, if, you]', '[you, sometimes, like]']\n",
      "scores=[-0.16737819  0.6454241   0.18464956  0.62508005 -0.1339095 ], predict: 1\n",
      "  bias=[ 0.05086858  0.16146143 -0.0737052   0.16508259  0.01069979]\n",
      " very bad (-0.1674): [ 0.00211978  0.          0.01011839  0.07290305  0.01357275  0.          0.04162535 -0.35858607]\n",
      "      bad (0.6454): [-0.02232614 -0.          0.33943877  0.07212346  0.05031346  0.         -0.07352789  0.11794101]\n",
      "  neutral (0.1846): [ 0.02878965 -0.          0.05414705 -0.10764705  0.10379078  0.          0.00159746  0.17767687]\n",
      "     good (0.6251): [ 0.0130075  -0.          0.20817901 -0.06775676  0.15483353 -0.         -0.09290115  0.24463533]\n",
      "very good (-0.1339): [-0.02252059  0.          0.06483873  0.03648314  0.025879    0.         -0.1072097  -0.14207986]\n",
      "\n",
      "4 ||| emerges as something rare , an issue movie that 's so honest and keenly observed that it does n't feel like one .\n",
      "['[as, something, rare]', \"[n't, feel, like]\", '[that, it, does]', '[an, issue, movie]', '[observed, that, it]', '[pad, emerges, as]', '[so, honest, and]', '[as, something, rare]']\n",
      "scores=[-0.17279181  0.4910419   0.2602584   0.6688226  -0.09136473], predict: 3\n",
      "  bias=[ 0.05086858  0.16146143 -0.0737052   0.16508259  0.01069979]\n",
      " very bad (-0.1728): [ 0.00715197  0.          0.0072389   0.02441037  0.01278327  0.          0.         -0.2752449 ]\n",
      "      bad (0.4910): [-0.07532664 -0.          0.24284123  0.02414934  0.04738691  0.         -0.          0.09052962]\n",
      "  neutral (0.2603): [ 0.097134   -0.          0.03873787 -0.03604383  0.09775366  0.          0.          0.1363819 ]\n",
      "     good (0.6688): [ 0.0438863  -0.          0.14893539 -0.02268722  0.14582744 -0.         -0.          0.18777813]\n",
      "very good (-0.0914): [-0.07598271  0.          0.04638691  0.01221577  0.02437372  0.         -0.         -0.10905821]\n",
      "\n",
      "2 ||| the film provides some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .\n",
      "['[great, insight, into]', '[the, neurotic, mindset]', '[of, all, comics]', '[top, of, the]', '[the, neurotic, mindset]', '[absolute, top, of]', '[the, neurotic, mindset]', '[the, film, provides]']\n",
      "scores=[-0.26129344  0.49295247  0.28590918  0.69701016 -0.1365319 ], predict: 3\n",
      "  bias=[ 0.05086858  0.16146143 -0.0737052   0.16508259  0.01069979]\n",
      " very bad (-0.2613): [ 0.00750296  0.          0.0062718   0.03428128  0.01182493  0.          0.         -0.37204298]\n",
      "      bad (0.4930): [-0.07902334 -0.          0.21039823  0.03391469  0.04383438  0.         -0.          0.12236707]\n",
      "  neutral (0.2859): [ 0.10190091 -0.          0.03356259 -0.05061899  0.09042521  0.          0.          0.18434466]\n",
      "     good (0.6970): [ 0.04604004 -0.          0.12903799 -0.03186134  0.13489497 -0.         -0.          0.25381592]\n",
      "very good (-0.1365): [-0.07971161  0.          0.04018973  0.01715551  0.02254646  0.         -0.         -0.14741178]\n",
      "\n",
      "4 ||| offers that rare combination of entertainment and education .\n",
      "['[offers, that, rare]', '[offers, that, rare]', '[that, rare, combination]', '[rare, combination, of]', '[that, rare, combination]', '[combination, of, entertainment]', '[entertainment, and, education]', '[offers, that, rare]']\n",
      "scores=[-0.10136705  0.37256235  0.20800754  0.55446076 -0.07042907], predict: 3\n",
      "  bias=[ 0.05086858  0.16146143 -0.0737052   0.16508259  0.01069979]\n",
      " very bad (-0.1014): [ 0.00627824  0.          0.0053797   0.          0.01061695  0.          0.         -0.17451052]\n",
      "      bad (0.3726): [-0.06612432 -0.          0.18047124  0.          0.03935647  0.         -0.          0.05739751]\n",
      "  neutral (0.2080): [ 0.08526757 -0.          0.02878865 -0.          0.08118779  0.          0.          0.08646873]\n",
      "     good (0.5545): [ 0.0385249  -0.          0.11068366 -0.          0.12111473 -0.         -0.          0.11905493]\n",
      "very good (-0.0704): [-0.06670024  0.          0.03447315  0.          0.02024321  0.         -0.         -0.06914499]\n",
      "\n",
      "3 ||| perhaps no picture ever made has more literally showed that the road to hell is paved with good intentions .\n",
      "['[picture, ever, made]', '[hell, is, paved]', '[that, the, road]', '[road, to, hell]', '[is, paved, with]', '[paved, with, good]', '[more, literally, showed]', '[is, paved, with]']\n",
      "scores=[-0.17606002  0.5133936   0.24532837  0.64747834 -0.105013  ], predict: 3\n",
      "  bias=[ 0.05086858  0.16146143 -0.0737052   0.16508259  0.01069979]\n",
      " very bad (-0.1761): [ 0.00250621  0.          0.00760071  0.01207049  0.01552285  0.          0.02307618 -0.28770503]\n",
      "      bad (0.5134): [-0.02639609 -0.          0.25497887  0.01194141  0.05754238  0.         -0.04076225  0.09462784]\n",
      "  neutral (0.2453): [ 0.03403787 -0.          0.04067406 -0.01782302  0.11870321  0.          0.0008856   0.14255582]\n",
      "     good (0.6475): [ 0.01537871 -0.          0.15637945 -0.01121842  0.17707966 -0.         -0.05150236  0.19627874]\n",
      "very good (-0.1050): [-0.026626    0.          0.04870541  0.00604048  0.02959724  0.         -0.05943471 -0.1139952 ]\n",
      "\n",
      "3 ||| steers turns in a snappy screenplay that curls at the edges ; it 's so clever you want to hate it .\n",
      "[\"[it, 's, so]\", '[screenplay, that, curls]', '[edges, ;, it]', '[the, edges, ;]', '[edges, ;, it]', '[want, to, hate]', '[pad, steers, turns]', '[clever, you, want]']\n",
      "scores=[-0.19445863  0.5601588   0.3399717   0.77287614 -0.13376707], predict: 3\n",
      "  bias=[ 0.05086858  0.16146143 -0.0737052   0.16508259  0.01069979]\n",
      " very bad (-0.1945): [ 0.00633196  0.          0.00823511  0.03510798  0.02184175  0.          0.02141474 -0.33825874]\n",
      "      bad (0.5602): [-0.06669006 -0.          0.27626088  0.03473256  0.08096623  0.         -0.03782745  0.11125524]\n",
      "  neutral (0.3400): [ 0.0859971  -0.          0.04406895 -0.05183968  0.16702385  0.          0.00082184  0.16760482]\n",
      "     good (0.7729): [ 0.03885451 -0.          0.16943179 -0.03262968  0.24916367 -0.         -0.0477943   0.23076756]\n",
      "very good (-0.1338): [-0.06727091  0.          0.05277065  0.01756922  0.04164542  0.         -0.05515553 -0.13402571]\n",
      "\n",
      "3 ||| but he somehow pulls it off .\n",
      "['[pad, but, he]', '[somehow, pulls, it]', '[pad, but, he]', '[but, he, somehow]', '[somehow, pulls, it]', '[somehow, pulls, it]', '[he, somehow, pulls]', '[he, somehow, pulls]']\n",
      "scores=[-0.17959923  0.5299138   0.27462357  0.70608544 -0.0636121 ], predict: 3\n",
      "  bias=[ 0.05086858  0.16146143 -0.0737052   0.16508259  0.01069979]\n",
      " very bad (-0.1796): [ 0.00500212  0.          0.00847518  0.          0.01402103  0.          0.         -0.25796613]\n",
      "      bad (0.5299): [-0.0526838  -0.          0.2843144   0.          0.05197521  0.         -0.          0.08484654]\n",
      "  neutral (0.2746): [ 0.06793597 -0.          0.04535364 -0.          0.10721879  0.          0.          0.12782039]\n",
      "     good (0.7061): [ 0.03069428 -0.          0.17437103 -0.          0.15994738 -0.         -0.          0.17599018]\n",
      "very good (-0.0636): [-0.05314267  0.          0.05430901  0.          0.02673374  0.         -0.         -0.10221197]\n",
      "\n",
      "3 ||| take care of my cat offers a refreshingly different slice of asian cinema .\n",
      "['[my, cat, offers]', '[of, my, cat]', '[my, cat, offers]', '[of, my, cat]', '[refreshingly, different, slice]', '[a, refreshingly, different]', '[slice, of, asian]', '[my, cat, offers]']\n",
      "scores=[-0.3766288   0.51862156  0.40245157  0.83114785 -0.17697985], predict: 3\n",
      "  bias=[ 0.05086858  0.16146143 -0.0737052   0.16508259  0.01069979]\n",
      " very bad (-0.3766): [ 0.00742605  0.          0.0068798   0.          0.01468515  0.          0.         -0.45648837]\n",
      "      bad (0.5186): [-0.07821331 -0.          0.23079468  0.          0.0544371   0.         -0.          0.15014166]\n",
      "  neutral (0.4025): [ 0.10085638 -0.          0.03681622 -0.          0.11229738  0.          0.          0.22618677]\n",
      "     good (0.8311): [ 0.04556811 -0.          0.1415472  -0.          0.16752353 -0.         -0.          0.31142643]\n",
      "very good (-0.1770): [-0.07889453  0.          0.04408581  0.          0.02800002  0.         -0.         -0.18087095]\n",
      "\n",
      "4 ||| this is a film well worth seeing , talking and singing heads and all .\n",
      "['[is, a, film]', '[talking, and, singing]', '[seeing, ,, talking]', '[pad, this, is]', '[well, worth, seeing]', '[is, a, film]', '[talking, and, singing]', '[pad, this, is]']\n",
      "scores=[-0.18830734  0.45466268  0.20266062  0.5973836  -0.06471679], predict: 3\n",
      "  bias=[ 0.05086858  0.16146143 -0.0737052   0.16508259  0.01069979]\n",
      " very bad (-0.1883): [ 2.8467036e-03  0.0000000e+00  5.9900377e-03  0.0000000e+00  1.0150291e-02  0.0000000e+00  2.0889752e-04 -2.5837186e-01]\n",
      "      bad (0.4547): [-0.02998232 -0.          0.20094603  0.          0.0376266   0.         -0.000369    0.08497998]\n",
      "  neutral (0.2027): [ 3.8662322e-02 -0.0000000e+00  3.2054778e-02 -0.0000000e+00  7.7619277e-02  0.0000000e+00  8.0168993e-06  1.2802143e-01]\n",
      "     good (0.5974): [ 0.0174681  -0.          0.12324093 -0.          0.11579128 -0.         -0.00046623  0.17626697]\n",
      "very good (-0.0647): [-0.03024346  0.          0.0383842   0.          0.01935345  0.         -0.00053803 -0.10237273]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for words, wids, tag in dev[:10]:\n",
    "    calc_predict_and_activations(wids, tag, words)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn4nlp",
   "language": "python",
   "name": "nn4nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
