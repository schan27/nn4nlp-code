{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7feb50116c10>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import os, sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define FFN language model\n",
    "class FFN_LM(nn.Module):\n",
    "    def __init__(self, nwords, emb_size, hid_size, num_hist, dropout):\n",
    "        super(FFN_LM, self).__init__()\n",
    "        self.embedding = nn.Embedding(nwords, emb_size)\n",
    "        # Use nn.Sequential to stack layers together\n",
    "        self.fnn = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                in_features=num_hist*emb_size,\n",
    "                out_features=hid_size,\n",
    "                bias=True),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(\n",
    "                in_features=hid_size, \n",
    "                out_features=nwords,\n",
    "                bias=True))\n",
    "    \n",
    "    def forward(self, words):\n",
    "        emb = self.embedding(words) # 3D tensor [batch_size x num_hist x emb_size]\n",
    "        # Note: size -1 is inferred from other dimensions\n",
    "        feat = emb.view(emb.size(0), -1)  # 2D tensor of size [batch_size x (num_hist*emb_size)]\n",
    "        logit = self.fnn(feat) # 2D tensor of size [batch_size x nwords]\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2\n",
    "EMB_SIZE = 128 # size of embedding\n",
    "HID_SIZE = 128 # size of hidden layer\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "# Initialize the model and the optimizer\n",
    "model = FFN_LM(nwords=nwords, emb_size=EMB_SIZE, hid_size=HID_SIZE, num_hist=N, dropout=0.2)\n",
    "if USE_CUDA:\n",
    "  model = model.cuda()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=0.001)  # Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "S = w2i[\"<s>\"]\n",
    "UNK = w2i[\"<unk>\"]\n",
    "def read_dataset(filename):\n",
    "  with open(filename, \"r\") as f:\n",
    "    for line in f:\n",
    "      yield [w2i[x] for x in line.strip().split(\" \")]\n",
    "    \n",
    "train = list(read_dataset(\"../data/ptb/train.txt\"))\n",
    "w2i = defaultdict(lambda: UNK, w2i)\n",
    "dev = list(read_dataset(\"../data/ptb/valid.txt\"))\n",
    "i2w = {v: k for k, v in w2i.items()}\n",
    "nwords = len(w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Peek at what the training set looks like\n",
    "train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Connection between `loss.backward()` and `optimizer.step()`](https://stackoverflow.com/questions/53975717/pytorch-connection-between-loss-backward-and-optimizer-step)\n",
    "\n",
    "* When initializing optimizer, we told it to update `model.parameters()`. Gradients are stored by the tensors themselves. After computing gradients for all tensors in the models, `optimizer.step()` does the parameter update.\n",
    "\n",
    "* `loss.backward()` computes gradient of loss w.r.t all the parameters in loss and stores them in `parameter.grad` attribute for every parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_variable(words):\n",
    "    data_type = torch.LongTensor\n",
    "    if USE_CUDA:\n",
    "        data_type = torch.cuda.LongTensor\n",
    "    var = torch.tensor(words).type(data_type)\n",
    "    return var\n",
    "    \n",
    "\n",
    "def calc_sent_loss(sent, ffn_model):\n",
    "    # Initial history = list of EOS tokens\n",
    "    hist = [S] * N\n",
    "    all_histories = []\n",
    "    all_targets = []\n",
    "    \n",
    "    # Step through sentence, incl. EOS token\n",
    "    for word in sent + [S]:\n",
    "        all_histories.append(list(hist))\n",
    "        all_targets.append(word)\n",
    "        hist = hist[1:] + [word]\n",
    "        \n",
    "    target = to_variable(all_targets)\n",
    "    logits = ffn_model(to_variable(all_histories))\n",
    "    loss = nn.functional.cross_entropy(\n",
    "        input=logits,\n",
    "        target=target)\n",
    "    return loss  # Scalar; same size as target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 5039/42068 [00:14<01:48, 340.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- finished 5000 sentences (word/sec=7129.843704671385)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 10089/42068 [00:25<00:47, 669.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- finished 10000 sentences (word/sec=8312.075084494212)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 15115/42068 [00:33<00:42, 636.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- finished 15000 sentences (word/sec=9510.632538733607)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 20058/42068 [00:46<01:01, 356.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- finished 20000 sentences (word/sec=9101.844729998478)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 25086/42068 [01:00<00:39, 430.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- finished 25000 sentences (word/sec=8789.046025066133)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 30060/42068 [01:12<00:32, 366.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- finished 30000 sentences (word/sec=8780.62272732762)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 35049/42068 [01:25<00:19, 368.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- finished 35000 sentences (word/sec=8648.183674085956)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 40082/42068 [01:39<00:04, 450.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- finished 40000 sentences (word/sec=8505.166196963566)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42068/42068 [01:43<00:00, 404.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: train loss/word=0.26074971731609586, ppl=1.297902782187288 (word/sec=8539.548232144462)\n",
      "iter 0: train loss/word=0.26370150697961214, ppl=1.30173957812544 (word/sec=57248.69215374463)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "last_dev = 1e20\n",
    "best_dev = 1e20\n",
    "\n",
    "for ITER in range(1):\n",
    "    random.shuffle(train)\n",
    "    \n",
    "    # Train the model\n",
    "    model.train()\n",
    "    train_words, train_loss = 0, 0.0\n",
    "    start = time.time()\n",
    "    \n",
    "    for sent_id, sent in tqdm(enumerate(train), total=len(train)):\n",
    "        my_loss = calc_sent_loss(sent, model)\n",
    "        train_loss += my_loss.item()\n",
    "        train_words += len(sent)\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        my_loss.backward()  # Backprop\n",
    "        optimizer.step()    # Gradient descent\n",
    "        \n",
    "        sent_num = sent_id+1\n",
    "        if sent_num % 5000 == 0:\n",
    "            # Show output every 5000 sentences\n",
    "            print(\"-- finished {} sentences (word/sec={})\".format(\n",
    "                sent_num,\n",
    "                train_words/(time.time()-start)))\n",
    "            \n",
    "    print(\"iter {}: train loss/word={}, ppl={} (word/sec={})\".format(\n",
    "        ITER,\n",
    "        train_loss/train_words,\n",
    "        math.exp(train_loss/train_words),\n",
    "        train_words/(time.time()-start)))\n",
    "\n",
    "    # Evaluate the model \n",
    "    model.eval()\n",
    "    dev_words, dev_loss = 0, 0.0\n",
    "    start = time.time()\n",
    "    for sent_id, sent in enumerate(dev):\n",
    "        my_loss = calc_sent_loss(sent, model)\n",
    "        dev_loss += my_loss.item()\n",
    "        dev_words += len(sent)\n",
    "        \n",
    "    # Keep track of dev accuracy; \n",
    "    # reduce learning rate if it got worse \n",
    "    if last_dev < dev_loss:\n",
    "        optimizer.learning_rate /= 2 \n",
    "    last_dev = dev_loss \n",
    "        \n",
    "    # Keep track of best dev accuracy;\n",
    "    # save the model only if it's the best one \n",
    "    if best_dev > dev_loss:\n",
    "        torch.save(model, \"model.pt\")\n",
    "            \n",
    "    print(\"iter {}: train loss/word={}, ppl={} (word/sec={})\".format(\n",
    "        ITER,\n",
    "        dev_loss/dev_words,\n",
    "        math.exp(dev_loss/dev_words),\n",
    "        dev_words/(time.time()-start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [`torch.multinomial`](https://pytorch.org/docs/stable/generated/torch.multinomial.html)\n",
    "\n",
    "* Returns a tensor where each row contains `num_samples` indices sampled from the multinomial probability distribution located in the corresponding row of tensor `input`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 100\n",
    "\n",
    "def generate_sent(ffn_model):\n",
    "    hist = [S] * N  # List of EOS symbols to start\n",
    "    sent = []\n",
    "    while True:\n",
    "        logits = ffn_model(to_variable([hist]))\n",
    "        # Get a probability distribution\n",
    "        m = nn.Softmax(dim=1)  # Dimension along which softmax will be computed\n",
    "        prob = m(logits)\n",
    "        # prob = nn.functional.softmax(logits)  \n",
    "        word = prob.multinomial(1).item()\n",
    "        if word == S or len(sent) == MAX_LEN:\n",
    "            break\n",
    "        sent.append(word)\n",
    "        hist = hist[1:] + [word]\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the nation talks over the stores who was outcry handling that that he left <unk> ensure\n",
      "japanese investors\n",
      "the bill holds the the proposal\n",
      "chicago a nine to kill boost poverty had half <unk> if you are rising will be a quick u.s. sounds step both the sun said mr. slow plenty of change\n",
      "for his night would have found banco bush college sports total\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    sent = generate_sent(model)\n",
    "    print(\" \".join([i2w[x] for x in sent]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn4nlp",
   "language": "python",
   "name": "nn4nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
