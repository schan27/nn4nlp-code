{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [`word2vec` Explained: Deriving Mikolov et al.â€™s Negative-Sampling Word-Embedding Method](https://arxiv.org/pdf/1402.3722.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbSkip(torch.nn.Module):\n",
    "    def __init__(self, nwords, emb_size):\n",
    "        # Use init function from superclass\n",
    "        super(WordEmbSkip, self).__init__()\n",
    "        \n",
    "        # Word embeddings\n",
    "        self.word_embedding = torch.nn.Embedding(nwords, emb_size, sparse=True)\n",
    "        torch.nn.init.xavier_uniform_(self.word_embedding.weight)\n",
    "        \n",
    "        # Context embeddings\n",
    "        self.context_embedding = torch.nn.Embedding(nwords, emb_size, sparse=True)\n",
    "        torch.nn.init.xavier_uniform_(self.context_embedding.weight)\n",
    "        \n",
    "    def forward(self, word_pos, context_positions, negative_sample=False):\n",
    "        embed_word = self.word_embedding(word_pos) # size = 1 x emb_size\n",
    "        embed_context = self.context_embedding(context_positions) # size = N x emb_size \n",
    "        \n",
    "        score = torch.matmul(\n",
    "            embed_context, \n",
    "            embed_word.transpose(dim0=1, dim1=0))\n",
    "        \n",
    "        # Only possible in dynamic framework\n",
    "        if negative_sample:\n",
    "            score *= -1\n",
    "        \n",
    "        # Why the -1 here?\n",
    "        obj = -1 * torch.sum(F.logsigmoid(score)) # Objective\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=3 #number of negative samples\n",
    "N=2 #length of window on each side (so N=2 gives a total window size of 5, as in t-2 t-1 t t+1 t+2)\n",
    "EMB_SIZE = 128 # The size of the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_location = \"embeddings.txt\" #the file to write the word embeddings to\n",
    "labels_location = \"labels.txt\" #the file to write the labels to\n",
    "\n",
    "# We reuse the data reading from the language modeling class\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "\n",
    "#word counts for negative sampling\n",
    "word_counts = defaultdict(int)\n",
    "\n",
    "S = w2i[\"<s>\"]\n",
    "UNK = w2i[\"<unk>\"]\n",
    "def read_dataset(filename):\n",
    "  with open(filename, \"r\") as f:\n",
    "    for line in f:\n",
    "      line = line.strip().split(\" \")\n",
    "      for word in line:\n",
    "        word_counts[w2i[word]] += 1\n",
    "      yield [w2i[x] for x in line]\n",
    "\n",
    "\n",
    "# Read in the data\n",
    "train = list(read_dataset(\"../data/ptb/train.txt\"))\n",
    "w2i = defaultdict(lambda: UNK, w2i)\n",
    "dev = list(read_dataset(\"../data/ptb/valid.txt\"))\n",
    "i2w = {v: k for k, v in w2i.items()}\n",
    "nwords = len(w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the word counts to the 3/4, normalize\n",
    "counts =  np.array([list(x) for x in word_counts.items()])[:,1]**.75\n",
    "normalizing_constant = sum(counts)\n",
    "word_probabilities = np.zeros(nwords)\n",
    "for word_id in word_counts:\n",
    "    word_probabilities[word_id] = word_counts[word_id]**.75/normalizing_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(labels_location, 'w') as labels_file:\n",
    "    for i in range(nwords):\n",
    "        labels_file.write(i2w[i] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "model = WordEmbSkip(nwords, EMB_SIZE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "data_type = torch.LongTensor\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    data_type = torch.cuda.LongTensor\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Combining multiple criterions](https://discuss.pytorch.org/t/how-to-combine-multiple-criterions-to-a-loss-function/348)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sent_loss(sent):\n",
    "    # Randomly sample words\n",
    "    all_neg_words = np.random.choice(\n",
    "        nwords,                # Values are in range(nwords)\n",
    "        size=2*N*K*len(sent),  # Number of values to draw\n",
    "        replace=True,\n",
    "        p=word_probabilities)  \n",
    "    \n",
    "    losses = []\n",
    "    for i, word in enumerate(sent):\n",
    "        before_words = [sent[x] if x >=0 else S for x in range(i-N, i)]\n",
    "        after_words = [sent[x] if x < len(sent) else S for x in range(i+1, i+N+1)]\n",
    "        \n",
    "        pos_words_tensor = torch.tensor(before_words+after_words).type(data_type)\n",
    "        neg_words = all_neg_words[i*K*2*N:(i+1)*K*2*N]\n",
    "        neg_words_tensor = torch.tensor(neg_words).type(data_type)\n",
    "        \n",
    "        target_word_tensor = torch.tensor([word]).type(data_type)\n",
    "        \n",
    "        # NOTE: Technically, we should ensure that neg words don't contain the context\n",
    "        # But that is very unlikely \n",
    "        \n",
    "        pos_loss = model(target_word_tensor, pos_words_tensor)\n",
    "        neg_loss = model(target_word_tensor, neg_words_tensor, negative_sample=True)\n",
    "        losses.append(pos_loss+neg_loss)   # Combining pos and neg loss\n",
    "    return torch.stack(losses).sum()  # Combining loss for all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started iter 0\n",
      "--finished 5000 sentences\n",
      "iter 0: train loss/word=9.013635528645256, ppl=8214.330489782013, time=249.01472759246826\n",
      "iter 0: dev loss/word=8.362590095129418, ppl=4283.775796565716, time=31.382995128631592\n"
     ]
    }
   ],
   "source": [
    "for ITER in range(1):\n",
    "    print(\"started iter %r\" % ITER)\n",
    "    \n",
    "    # Start training\n",
    "    random.shuffle(train)\n",
    "    train_words, train_loss = 0, 0.0\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    \n",
    "    for sent_id, sent in enumerate(train[:5000]):\n",
    "        my_loss = calc_sent_loss(sent)\n",
    "        train_loss += my_loss.item()\n",
    "        train_words += len(sent)\n",
    "        \n",
    "        # Take step after calculating loss for all words in sent\n",
    "        optimizer.zero_grad()  # Zero the gradients \n",
    "        my_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (sent_id+1) % 5000 == 0:\n",
    "            print(\"--finished {} sentences\".format(sent_id+1))\n",
    "    \n",
    "    train_ppl = float('inf') if train_loss / train_words > 709 else math.exp(train_loss / train_words)\n",
    "    print(\"iter {}: train loss/word={}, ppl={}, time={}\".format(\n",
    "        ITER,\n",
    "        train_loss/train_words,\n",
    "        train_ppl,\n",
    "        time.time()-start))\n",
    "    \n",
    "    # Evaluate on dev set \n",
    "    dev_words, dev_loss = 0, 0.0\n",
    "    start = time.time()\n",
    "    model.eval() \n",
    "    for sent_id, sent in enumerate(dev[:5000]):\n",
    "        my_loss = calc_sent_loss(sent)\n",
    "        dev_loss += my_loss.item()\n",
    "        dev_words += len(sent)\n",
    "    \n",
    "    # Why 709?\n",
    "    dev_ppl = float('inf') if dev_loss / dev_words > 709 else math.exp(dev_loss / dev_words)\n",
    "    print(\"iter {}: dev loss/word={}, ppl={}, time={}\".format(\n",
    "        ITER,\n",
    "        dev_loss/dev_words,\n",
    "        dev_ppl,\n",
    "        time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn4nlp",
   "language": "python",
   "name": "nn4nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
